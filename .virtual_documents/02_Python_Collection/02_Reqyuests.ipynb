


pip install requests





import requests
from bs4 import BeautifulSoup


PATH = 'https://finance.naver.com/'
resp = requests.get(PATH) #주소 넣기에 대한 resp(응답)


resp


resp?


requests.get?


resp.text


print(resp.text)


# dom 객체로 변환~


src = resp.text


soup = BeautifulSoup(src, 'lxml')
print(soup)





# 뉴스 목록 찾기
srclist = soup.select('.section_strategy li a')
srclist


srclists = soup.select('.news_area li a')
srclists


len(srclist)
len(srclists)


srclists[0]


for i in soup.select('.div.section'):
    i = item.find('','price').text


# 첫번째 뉴스 제목 추출
srclists[0].text


scrlist[0]['href']





url = scrlist[0]['href']
PATH + url


# naver.com// 으로 /가 2개 -> 고쳐줘야함


from urllib.parse import urljoin


urljoin(PATH, url)





srclists = soup.select('.news_area li a')
for i in srclists:
    title = i.find(


news_title = []
news_url= []

for i in srclist:
    title = i.text
    url = urljoin(PATH, i['href'])
    print(title, url)
    news_title.append(title)
    news_url.append(url)


news_title


news_url





import pandas as pd


df = pd.DataFrame({'제목':news_title, '주소':news_url})


df


print(df)





df.to_excel('output/naver.xlsx', index = False) #<- 반드시 확장자 넣기





# 파일명에 저장시각을 추가해보자
import time


today = time.localtime()
today


time.ctime()


df.to_excel(f'output/{today.tm_year}_{today.tm_mon}_{today.tm_mday}_naver.xlsx', index = False)


# 스트링 포매팅으로 날짜 포맷을 바꿔보자
# 2024-06-20
'%d-%-2d-%-2d'%(today.tm_year, today.tm_mon, today.tm_mday)


filename = '%d-%-2d-%-2d'%(today.tm_year, today.tm_mon, today.tm_mday)
excel_name = filename + '.xlsx'
csv_name = filename +'.csv'
excel_name, csv_name


df.to_csv(f'output/{csv_name}')














import requests
from bs4 import BeautifulSoup
import pandas as pd


PATH = 'https://www.melon.com/chart/index.htm'
resp = requests.get(PATH)
resp


resp.text


# 유저 에이전트 등록이 안되어있어서 내용이 나오지 않는 것 !


info = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}
info


requests.get? #kwargs = keyward arguments


resp = requests.get(PATH, headers = info)
resp


html_src = resp.text
html_src


soup = BeautifulSoup(html_src,'lxml')
soup





# 곡이름 찾기


songlist = soup.select('.ellipsis.rank01 a')
songlist


song = [i.text for i in soup.select('.ellipsis.rank01 a')]
len(song)


len(songlist)





artist = soup.select('.ellipsis.rank02>a')
artist


len(artist)


artist = []
for i in soup.select('.ellipsis.rank02>a'):
    artist.append(i.text)
print(len(artist))
print(artist)


# 굳이 새로 리스트를 추가한 이유는 곡은 100갠데 아티스트는 106개라서 100개로 맞춰주기 위해(여럿이 한 곡 부른게 있어서)


len(soup.select('.checkEllipsis'))





name = [i.text for i in soup.select('.checkEllipsis')]
len(name)


# 순위 인덱스 만들기
rank = list(range(1,101))
rank


# 판다스로 데이터프레임 만들기 Column : data
songDF = pd.DataFrame({ '순위': rank,
             '노래제목': song,
             '가수명': name})
songDF


songDF.to_excel('output/melon100.xlsx', index = False) # 이렇게 해야 맨 왼쪽에 01234 안나옴








import requests


param = {'name': 'Amy', 'age': '20', 'address': 'Seoul'}


#get 방식
resp1 = requests.get('http://httpbin.org/get', params = param) # 인터페이스가 파라미터
print(resp1)
print(resp1.text)


# Post 방식
resp2 = requests.get('http://httpbin.org/post', params = param) #비밀리에 보내는거라 url이 파라미터를 안받음
print(resp2)
print(resp2.text)


# Post 방식
resp2 = requests.post('http://httpbin.org/post', data = param) 
print(resp2)
print(resp2.text)


# args 에 값이 없고  form에 있음 
# html로 회원가입 할 때 id, pw, login 같은 버튼들, 입력창들을 form 요소라고 함: 이 요소로 보내야만 post를 사용할 수 있음





from bs4 import BeautifulSoup


PATH = 'https://search.naver.com/search.naver'


keyword = input('검색어를 입력하세요: ')


resp = requests.get(PATH, params = {'query':keyword} )
resp


html_src = resp.text
html_src


soup = BeautifulSoup(html_src,'lxml')
soup


n_title = soup.select('.news_contents>a')
n_title


news_title = []
news_url= []

for i in ntitle:
    title = i.text
    url = urljoin(PATH, i['href'])
    print(title, url)
    news_title.append(title)
    news_url.append(url)


result = soup.select('.news_tit')

print(result)


for i in result:
    print(i.text,i['href'])


findall('a',class_newstitle?) 3:37? 즘,,





response = requests.get('https://korean.visitkorea.or.kr/')
html_src = response.text


html_src


soup = BeautifulSoup(html_src, 'lxml')
soup


pop_list = soup.select('.popular span a')
print(pop_list)


# 동적 웹페이지라서 셀레니움 사용 없이는 받아올 수가 없음















