





import requests, json, pandas as pd


URL = 'http://www.krei.re.kr:18181/chart/main_chart/index/kind/W/sdate/2019-01-01/edate/2019-12-31'
resp = requests.get(URL)
resp


data = resp.text
print(type(data),len(data))
print(data)


data


data2 = resp.json()
print(type(data2),len(data2))
print(data2)


data2


ddd = []
for i in data2:
    id = i.get(id)
    data = i.get(data)
    symbol = i.get(symbol)
    open = i.get(open)
    close = i.get(close)
    ddd.append(id)
    ddd.append(data)
    ddd.append(symbol)
    ddd.append(open)
    ddd.append(close)


id_, date, symbol, open_, close= [], [], [], [], []

for item in data2 :
    id_.append(item['id'])
    date.append(item['date'])
    symbol.append(item['symbol'])
    open_.append(item['open'])
    close.append(item['close'])


df = pd.DataFrame({
    'id' : id_,
    'date' : date,
    'symbol' :symbol,
    'open' : open_,
    'close' : close,
})


df





import requests
from bs4 import BeautifulSoup


#api 키
#QZDKs%2FoGt0QFHXINoMJZHoUIBWan%2B1sahzLzfJ8rNkgzKP9z3MLkx02n%2BRNnS0k5%2BPDK2rbcr3iOvz%2BZ9nK1Vg%3D%3D
date = '201512'
region_code = '11110'
key = 'QZDKs%2FoGt0QFHXINoMJZHoUIBWan%2B1sahzLzfJ8rNkgzKP9z3MLkx02n%2BRNnS0k5%2BPDK2rbcr3iOvz%2BZ9nK1Vg%3D%3D'
url = f'http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD={region_code}&DEAL_YMD={date}&serviceKey={key}'

resp = requests.get(url)
textsrc = resp.text
textsrc


# 데이터 형식  보통xml, json -> 둘 중 한개


soup = BeautifulSoup(textsrc,'xml')
soup


# xml 은 id 와 class 가 없어서 select 써야 함


items = soup.select('response > body > items > item')
items


# items = soup.find_all
# items


items[0]



a = items[0].select('거래금액')
print(type(a),a) # 태그 요소 -> 1개로 뽑으려면 find 나 select_one으로 뽑아야 함


a = items[0].select_one('거래금액')
print(type(a),a) # tag 객체로 만들었으니 .text 가능 !


a.text


#Varchar 아니고 공간 차지하는 이유 -> 데이터 수집 속도 때문에





items = soup.select('response > body > items > item')
items


item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']

for item in items:
    for tag in item_list:
        print(item.select_one(tag).text.strip(), end=' ')
    print()





def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text


item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']
for item in items:
    for tag in item_list:
        print(default_text(item.select_one(tag), ''), end=' ')
    print()





item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']
data_list = []

for item in items:
    data_list.append([ default_text(item.select_one(tag), '') for tag in item_list]) # value를 행기준으로 맞춤, 110 X 11
print(data_list)

df = pd.DataFrame(data_list, columns=item_list)
print('='*50)
print(df)
df.to_csv('./sample/아파트매매실거래자료수집.csv', index=False, encoding='cp949') 





item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']

for item in items:
    for tag in item_list:
        print(item.select_one(tag).text.strip(), end=' ')
    print()

def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text

for item in items:
    for tag in item_list:
        print(default_text(item.select_one(tag), ''), end=' ')
    print()








import requests, datetime, dateutil, pandas as pd
from bs4 import BeautifulSoup

def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text
    
start_datetime = datetime.datetime(2023, 1, 1)

data_list = []
item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']

while True:
    date = start_datetime.strftime('%Y%m') #202301
    
    if date >= '202309': #none일떄 break로 해도 됨!(이건 그당시 마지막 업데이트 일자에 맞춰서23년 9월로 작성하신 것)
        break
        
    region_code = '11110'
    
    apikey = 'QjITnZtxSg5%2Bhzh%2BWR8hYLMstCDRuf1REcb5E59648Wy77%2B7z8aQBHgv95ylOhyoP31mFZWlyiqd2TrMu7HTuw%3D%3D'
    url = f'http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev?LAWD_CD={region_code}&DEAL_YMD={date}&serviceKey={apikey}'

    res = requests.get(url)
    textsrc = res.text

    soup = BeautifulSoup(textsrc, 'xml')
    items = soup.select('response > body > items > item') 
    
    print('='*10, date, '='*10)
    
    for item in items:
        data_list.append([ default_text(item.select_one(i), '') for i in item_list]) # value를 행기준으로 맞춤, 110 X 11
        print(data_list)
    
    start_datetime = start_datetime + dateutil.relativedelta.relativedelta(months=1)  # += 1달 

df = pd.DataFrame(data_list, columns=item_list)
df.to_csv('./sample/아파트매매 실거래자료.csv', index=False, encoding='cp949') 

# 작성은 못해도.. 작성된걸 보고 이해는 할 수 있을 것 같습니다..!





import requests, datetime, dateutil, pandas as pd
from bs4 import BeautifulSoup

def default_text(node, text):
    if node != None:
        return node.text.strip()
    else:
        return text

item_list = ['거래금액', '건축년도', '년', '법정동', '아파트', '월', '일', '전용면적', '지번', '지역코드', '층']
data_list = []        
lawd_cds = [11140, 11170, 11200]
# lawd_cds = [11110, 11140, 11170, 11200, 11215, 11230, 11260, 11290, 11305,
#        11320, 11350, 11380, 11410, 11440, 11470, 11500, 11530, 11545,
#        11560, 11590, 11620, 11650, 11680, 11710, 11740]

start_datetime = datetime.datetime(2023, 1, 1)

while True:
    apikey = 'QjITnZtxSg5%2Bhzh%2BWR8hYLMstCDRuf1REcb5E59648Wy77%2B7z8aQBHgv95ylOhyoP31mFZWlyiqd2TrMu7HTuw%3D%3D'    
    
    date = start_datetime.strftime('%Y%m') #202301
    if date >= '202309':
        break
        
    for lawd_cd in lawd_cds:
        print('='*5, date, lawd_cd, '='*5)
        url = f'http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev?LAWD_CD={lawd_cd}&DEAL_YMD={date}&serviceKey={apikey}'

        res = requests.get(url)
        textsrc = res.text

        soup = BeautifulSoup(textsrc, 'xml')
        items = soup.select('response > body > items > item') 
           
        for item in items:
            data_list.append([ default_text(item.select_one(i), '') for i in item_list]) # value를 행기준으로 맞춤, 110 X 11
        print(data_list)
        
        start_datetime = start_datetime + dateutil.relativedelta.relativedelta(months=1) #  += 1달

df = pd.DataFrame(data_list, columns=item_list)
df.to_csv('./sample/아파트매매 실거래자료.csv', index=False, encoding='cp949')
print('저장 완료')


# while 문 사용 이유: 특정 시점부터 최근까지(끝을 모르는 경우에 사용)





import requests, time, os, json
from html import unescape


# input
client_id = 'LxpmpyREGJHpBqlAwics'
client_secret = '_fucEeF4x6'

queries = ['전주 여행', '경주 여행']
goal_page = 5


# setting
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"

headers = {"User-Agent": user_agent,
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret}


file_name = './sample/naver_kin.txt'

with open(file_name, 'w', encoding='utf-8') as f :
    f.write('query\tno\ttitle\tlink\tdescription\ttotal_text\n')


url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + queries[0] + "&start=" + str(1) #3:25 왜 쿼리가 0인지
response = requests.get(url, headers=headers)
response


print(response.text) # key value 형식임 -> 제이슨 형식


json.loads(response.text) #딕셔너리에 리스트가 묶여있고 그 안에 다시 딕셔너리가 있음, items 안을 추출


json.loads(response.text)['items']


elements = json.loads(response.text)['items']
elements[0]


def get_list(query, page):
    print('='*5, query, page, '='*5)
    url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + query + "&start=" + str(page+1)
    response = requests.get(url, headers=headers)
    elements = json.loads(response.text)['items']

    for i, elm in enumerate(elements): # 값ㅇ ㅣ몇번째인지 인덱스를 주기 위해 이뮬레이트 사용
        title = elm['title'].replace("<b>", "").replace("</b>", "") # 볼드 없애기 위해 사용
        title = unescape(title) # escape된 문자를 unescape문자로 변경 : escape 문자가 들어있는 경우가 있어 사용(" 같은거 대체 용어를 "로 다시 돌리기)
        link = elm['link']
        description = unescape(elm['description'].replace("<b>", "").replace("</b>", "")) # 마찬가지로 볼드 없앰
       # description = unescape(description) # 혹시 escape 문자 있으면 빼주기 -> 이미 위에 있어서 불필요
        
        print([query, (page*100)+(i+1), title, link, description, title+" "+description]) # 변수 들어온 값 하나하나를 다 표시

        with open(file_name, 'a', encoding='utf-8') as f: # overwrite 안되도록 add할 것
            f.write( f'{query}\t{(page*100)+(i+1)}\t{title}\t{link}\t{description}\t{title+" "+description}\n')

    return


for query in queries: #쿼리(전주, 경주 여행) 수대로 봄
    for page in range(goal_page): # 5번 페이지까지 지정함(전주, 경주 5번페이지까지 총 10페이지, 한페이지당 100개)
        kin_list = get_list(query, page)
        time.sleep(0.5) #웹페이지 크롤링 매너 최소 6초









