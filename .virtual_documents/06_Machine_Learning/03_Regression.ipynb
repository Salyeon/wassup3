


import warnings
warnings.filterwarnings('ignore')





import seaborn as sns

DF = sns.load_dataset('mpg')








DF.info()


DF.head(3)








DF1 = DF[['mpg', 'cylinders', 'displacement', 'weight']]

DF1.head(3)








import matplotlib.pyplot as plt

plt.figure(figsize = (9, 6))
plt.scatter(x = DF1.weight, y = DF1.mpg, s = 30)
plt.show()





fig = plt.figure(figsize = (9, 6))
sns.regplot(x = 'weight', y = 'mpg', data = DF1, fit_reg = True)
plt.show()





sns.pairplot(DF1)  
plt.show()








DF1.corr()


from scipy import stats

stats.pearsonr(DF1.mpg, DF1.weight)[0]





from scipy import stats

stats.pearsonr(DF1.mpg, DF1.displacement)[0]





from scipy import stats

stats.pearsonr(DF1.mpg, DF1.cylinders)[0]








from sklearn.model_selection import train_test_split
train_test_split?


from sklearn.model_selection import train_test_split

X = DF1[['weight']]
y = DF1['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)








from sklearn.linear_model import LinearRegression

SR = LinearRegression()
SR.fit(X_train, y_train)





print('weight(w) : ', SR.coef_)
print('bias(b) : ', SR.intercept_)





SR.score(X_test, y_test)








from sklearn.metrics import mean_squared_error

y_hat_Xtest = SR.predict(X_test)

mean_squared_error(y_test, y_hat_Xtest)








# 실제 정답데이터와 테스트셋 예측값에 대한 분포를 비교해보자
SR_y = y
SR_y_test = y_test
SR_y_hat_Xtest = SR.predict(X_test)

plt.figure(figsize = (9, 6))
ax1 = sns.distplot(SR_y, hist = False, label = 'y') # 전체데이터셋 종속변수(정답데이터)의 분포
ax2 = sns.distplot(SR_y_test, hist = False, label='y_test', ax = ax1) #  테스트셋 종속변수(정답데이터)의 분포
ax3 = sns.distplot(SR_y_hat_Xtest, hist = False, label='y_hat_Xtest', ax = ax1) # 테스트셋 독립변수(미훈련한 30%의 설명데이터)에 대한 회귀예측값의 분포

plt.legend()
plt.ylim(0, 0.07)
plt.show()


파랑 - 전체
주황 - 일부 데이터
초록 - 일부의 20%로 예측했는데 값이 튐








DF2 = DF[['mpg', 'cylinders', 'horsepower', 'weight']]

DF2.head(3)








from sklearn.model_selection import train_test_split

X = DF2[['weight']]
y = DF2['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)








from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree = 2, include_bias = False)
X_train_poly = poly.fit_transform(X_train) # X전체로 fit하면 안됨, 정답을 미리 알려주는 꼴

print('변환 전 데이터: ', X_train.shape)
print('2차항 변환 데이터: ', X_train_poly.shape)





from sklearn.linear_model import LinearRegression

PR = LinearRegression()
PR.fit(X_train_poly, y_train)





import numpy as np
np.set_printoptions(suppress = True, precision = 10)

print('weight(w) : ', PR.coef_)
print('bias(b) : ', '%.8f' % PR.intercept_)





X_test_poly = poly.transform(X_test)

PR.score(X_test_poly, y_test)








from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, PR.predict(X_test_poly))








PR_y_hat_Xtest = PR.predict(X_test_poly)

plt.figure(figsize=(9, 6))
plt.plot(X_train, y_train, 'o', label = 'Train Data')
plt.plot(X_test, PR_y_hat_Xtest, 'r+', label = 'Predicted Value')
plt.legend(loc='best')
plt.xlabel('weight')
plt.ylabel('mpg')
plt.show()





PR_y = y
PR_y_test = y_test
PR_y_hat_Xtest = PR.predict(X_test_poly)

plt.figure(figsize = (9, 6))
ax1 = sns.distplot(PR_y, hist=False, label="y")
ax2 = sns.distplot(PR_y_test, hist=False, label="y_test", ax=ax1)
ax3 = sns.distplot(PR_y_hat_Xtest, hist=False, label="y_hat_Xtest", ax=ax1)

plt.legend()
plt.ylim(0, 0.07)
plt.show()








DF3 = DF[['mpg', 'cylinders', 'displacement', 'weight']]

DF3.head(3)





from sklearn.model_selection import train_test_split

X = DF3[['displacement', 'weight']]
y = DF3['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)








from sklearn.linear_model import LinearRegression

MR = LinearRegression()   
MR.fit(X_train, y_train)





print('weight(w) : ', MR.coef_)
print('bias(b) : ', '%.8f' % MR.intercept_)





MR.score?


MR.score(X_test, y_test)


# 다중회귀모형은 조정된 결정계수를 봐야한다.
# sklean lR에서 기능을 제공하지 않으므로 추가 계산을 해야함  또는 statsmodel api를 활용할 수있음
1 - (1-MR.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)








from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, MR.predict(X_test))





MR_y = y
MR_y_test = y_test
MR_y_hat_Xtest = MR.predict(X_test)

plt.figure(figsize = (9, 6))
ax1 = sns.distplot(MR_y, hist = False, label = 'y')
ax2 = sns.distplot(MR_y_test, hist = False, label = 'y_test')
ax3 = sns.distplot(MR_y_hat_Xtest, hist = False, label='y_hat_Xtest', ax = ax1)

plt.ylim(0, 0.07)
plt.legend()
plt.show()





plt.figure(figsize = (9, 6))
ax1 = sns.distplot(y_test, hist = False, label = 'y_test')
ax2 = sns.distplot(SR_y_hat_Xtest, hist = False, label='y_hat_simple', ax = ax1) # 단순회귀
ax3 = sns.distplot(PR_y_hat_Xtest, hist = False, label='y_hat_poly2', ax = ax1) # 다항회귀 2차
ax4 = sns.distplot(MR_y_hat_Xtest, hist = False, label='y_hat_multi', ax = ax1) # 다중회귀
plt.legend()
plt.ylim(0, 0.07)
plt.show()





import warnings
warnings.filterwarnings('ignore')








import pandas as pd

url = 'https://raw.githubusercontent.com/rusita-ai/pyData/master/Insurance.csv'
DF = pd.read_csv(url)

DF.info()


DF.head(3)








import matplotlib.pyplot as plt
import seaborn as sns 





plt.figure(figsize = (9, 6))
sns.distplot(DF.expenses,
             hist = True,
             kde = True)
plt.show()


plt.figure(figsize = (9, 6))
sns.boxplot(y = 'expenses', data = DF)
plt.show()





plt.figure(figsize = (9, 6))
sns.boxplot(x = 'sex', y = 'expenses', data = DF)
plt.show()


DF.sex.value_counts()





plt.figure(figsize = (9, 6))
sns.boxplot(x = 'children', y = 'expenses', data = DF)
plt.show()


DF.children.value_counts()





plt.figure(figsize = (9, 6))
sns.boxplot(x = 'smoker', y = 'expenses', data = DF)
plt.show()


DF.smoker.value_counts()





plt.figure(figsize = (9, 6))
sns.boxplot(x = 'region', y = 'expenses', data = DF)
plt.show()


DF.region.value_counts()





plt.figure(figsize = (9, 6))
sns.distplot(DF.bmi,
             hist = True,
             kde = True)
plt.show()





plt.figure(figsize = (9, 6))
sns.scatterplot(x = DF.bmi, y = DF.expenses)
plt.show()








DF.info()





from sklearn.preprocessing import LabelEncoder

encoder1 = LabelEncoder()
DF['sex'] = encoder1.fit_transform(DF.sex)


encoder2 = LabelEncoder()
DF['smoker'] = encoder2.fit_transform(DF.smoker)


encoder3 = LabelEncoder()
DF['region'] = encoder3.fit_transform(DF.region)





DF.info()





DF.head()





from sklearn.model_selection import train_test_split

X = DF[['age', 'sex']]
y = DF['expenses']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)





from sklearn.linear_model import LinearRegression

RA = LinearRegression()
RA.fit(X_train, y_train)





y_hat = RA.predict(X_test)





from sklearn.metrics import mean_squared_error
import numpy as np

mse1 = mean_squared_error(y_test, y_hat)

np.sqrt(mse1)





import pandas as pd

url = 'https://raw.githubusercontent.com/rusita-ai/pyData/master/Insurance.csv'
DF = pd.read_csv(url)

DF.info()





from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(DF,
                                       test_size = 0.3,
                                       random_state = 2045)

train_set.shape, test_set.shape


train_set.info()





import statsmodels.formula.api as smf

Model_1 = smf.ols(formula = 'expenses ~ age + sex',
                  data = train_set).fit()





y_hat_1 = Model_1.predict(test_set[['age', 'sex']])





mse2 = mean_squared_error(test_set.expenses, y_hat_1)

np.sqrt(mse2)





print('sklearn     :', np.sqrt(mse1))
print('statsmodels :', np.sqrt(mse2))








import warnings
warnings.filterwarnings('ignore')





import numpy as np

def sigmoid(x):
    y_hat = 1 / (1 + np.exp(-x))
    return y_hat





sigmoid(0)


sigmoid(100000000)


sigmoid(-100000000)





import matplotlib.pyplot as plt

n = np.linspace(-10.0, 10.0, 2000)

plt.figure(figsize = (9, 6))
plt.plot(n, sigmoid(n))
plt.show()





import warnings
warnings.filterwarnings('ignore')








import pandas as pd

DF = pd.read_csv('https://raw.githubusercontent.com/rusita-ai/pyData/master/Default.csv')

DF.info()
#파산에 대한 자료


DF.head()








DF.default.value_counts()





import matplotlib.pyplot as plt

plt.figure(figsize = (9, 6))
plt.boxplot([DF[DF.default == 'No'].balance,
             DF[DF.default == 'Yes'].balance],
            labels = ['No', 'Yes'])
plt.show()








X = DF[['balance']]
y = DF['default']








from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)








from sklearn.linear_model import LogisticRegression

Model_lr = LogisticRegression()
Model_lr.fit(X_train, y_train)





y_hat = Model_lr.predict(X_test)





y_hat 








from sklearn.metrics import confusion_matrix
confusion_matrix?


confusion_matrix(y_test, y_hat) # y_test = 실제값





from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_hat, labels = ['Yes','No'])





from sklearn.metrics import accuracy_score, precision_score, recall_score

print(accuracy_score(y_test, y_hat))
print(precision_score(y_test, y_hat, pos_label = 'No'))
print(recall_score(y_test, y_hat, pos_label = 'No'))





from sklearn.metrics import accuracy_score, precision_score, recall_score

print(accuracy_score(y_test, y_hat))
print(precision_score(y_test, y_hat, pos_label = 'Yes'))
print(recall_score(y_test, y_hat, pos_label = 'Yes'))





from sklearn.metrics import f1_score

f1_score(y_test, y_hat, pos_label = 'No')





from sklearn.metrics import f1_score

f1_score(y_test, y_hat, pos_label = 'Yes')





from sklearn.metrics import classification_report

print(classification_report(y_test, y_hat, 
                            target_names = ['No', 'Yes'], # 앞에가 positive라서 순서가 중요
                            digits = 5)) # 소수점 자리수





import warnings
warnings.filterwarnings('ignore')











import numpy as np

y = 1
y_hat = 1

-y * np.log(y_hat)





y = 1
y_hat = 0.0001

-y * np.log(y_hat)





y = 0
y_hat = 0

-(1 - y) * np.log(1 - y_hat)





y = 0
y_hat = 0.9999

-(1 - y) * np.log(1 - y_hat)








# 각 사건이 발생할 확률
A = 0.9
B = 0.5
C = 0.1
# 가장 발생할 확률이 낮은 C가 정보량이 가장 높음
print('%.3f' % -np.log(A), '%.3f' % -np.log(B), '%.3f' % -np.log(C))





Alphago = 0.999
Apes = 0.001
# 알파고가 이겼을 때 놀람의 정도보다, 침팬지가 이겼을 때 놀람의 정도가 더 크다.
print('%.3f' % -np.log(Alphago), '%.3f' % -np.log(Apes))











P1 = 0.5
P2 = 0.5

-P1 * np.log(P1) - P2 * np.log(P2)





P1 = 0.999
P2 = 0.001

-P1 * np.log(P1) - P2 * np.log(P2)



